{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "G6aVmRmzFAy4"
      },
      "source": [
        "# **Read Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:10:52.192963900Z",
          "start_time": "2023-12-11T21:10:49.161403Z"
        },
        "id": "BNUlprEcFAy8"
      },
      "outputs": [],
      "source": [
        "## Importing Packages\n",
        "import math\n",
        "import nltk\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:11:41.392405500Z",
          "start_time": "2023-12-11T21:11:38.818460900Z"
        },
        "id": "Ti9SK9ToFAy9"
      },
      "outputs": [],
      "source": [
        "file_path = r'D:\\Courses\\codes\\Py\\GRAMSAUTO\\DATA\\en_US\\data\\en_US.twitter.txt'\n",
        "\n",
        "# Open the file in read mode ('r')\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    # Read the entire content of the file into a string\n",
        "    contentB = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:11:47.689759900Z",
          "start_time": "2023-12-11T21:11:47.686513700Z"
        },
        "id": "8_byevuIFAy-"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "content = re.sub(r'[^a-zA-Z\\s]', '', contentB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:11:49.657001700Z",
          "start_time": "2023-12-11T21:11:49.645138400Z"
        },
        "id": "Ck4EaZLlFAy-"
      },
      "outputs": [],
      "source": [
        "def preprocess_pipeline(data) -> 'list':\n",
        "\n",
        "    # Split by newline character\n",
        "    sentences = data.split('\\n')\n",
        "\n",
        "        # Remove leading and trailing spaces\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "\n",
        "    # Drop Empty Sentences\n",
        "    sentences = [s for s in sentences if len(s) > 0]\n",
        "\n",
        "    # Empty List to hold Tokenized Sentences\n",
        "    tokenized = []\n",
        "\n",
        "    # Iterate through sentences\n",
        "    for sentence in sentences:\n",
        "\n",
        "        # Convert to lowercase\n",
        "        sentence = sentence.lower()\n",
        "\n",
        "        # Convert to a list of words\n",
        "        token = nltk.word_tokenize(sentence)\n",
        "\n",
        "        # Append to list\n",
        "        tokenized.append(token)\n",
        "\n",
        "    return tokenized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:13:48.883398800Z",
          "start_time": "2023-12-11T21:11:52.920673500Z"
        },
        "id": "m-W2ElXcFAy-"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences = preprocess_pipeline(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:14:50.883450500Z",
          "start_time": "2023-12-11T21:14:50.740738800Z"
        },
        "id": "nbgia__UFAy_",
        "outputId": "55364258-7d8a-48b1-bf85-373eda44a54d"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:14:55.487883400Z",
          "start_time": "2023-12-11T21:14:53.358607800Z"
        },
        "id": "gCj0nbFtFAy_"
      },
      "outputs": [],
      "source": [
        "## Obtain Train and Test Split\n",
        "train, test = train_test_split(tokenized_sentences, test_size=0.2, random_state=42)\n",
        "\n",
        "## Obtain Train and Validation Split\n",
        "train, val = train_test_split(train, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:14:57.741822200Z",
          "start_time": "2023-12-11T21:14:57.727433800Z"
        },
        "id": "00cqMNqWFAzA"
      },
      "outputs": [],
      "source": [
        "def count_the_words(sentences) -> 'dict':\n",
        "\n",
        "  # Creating a Dictionary of counts\n",
        "  word_counts = {}\n",
        "\n",
        "  # Iterating over sentences\n",
        "  for sentence in sentences:\n",
        "\n",
        "    # Iterating over Tokens\n",
        "    for token in sentence:\n",
        "\n",
        "      # Add count for new word\n",
        "      if token not in word_counts.keys():\n",
        "        word_counts[token] = 1\n",
        "\n",
        "      # Increase count by one\n",
        "      else:\n",
        "        word_counts[token] += 1\n",
        "\n",
        "  return word_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:15:07.203364Z",
          "start_time": "2023-12-11T21:14:59.454612300Z"
        },
        "id": "c6mOZ8YqFAzA",
        "outputId": "19fe2c84-d865-4bc5-8ddd-ee7898a61070"
      },
      "outputs": [],
      "source": [
        "word_count = count_the_words(tokenized_sentences)\n",
        "word_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:15:08.011472900Z",
          "start_time": "2023-12-11T21:15:07.999122700Z"
        },
        "id": "I8LQg-cXFAzB"
      },
      "outputs": [],
      "source": [
        "def handling_oov(tokenized_sentences, count_threshold) -> 'list':\n",
        "\n",
        "  # Empty list for closed vocabulary\n",
        "  closed_vocabulary = []\n",
        "\n",
        "  # Obtain frequency dictionary using previously defined function\n",
        "  words_count = count_the_words(tokenized_sentences)\n",
        "\n",
        "  # Iterate over words and counts\n",
        "  for word, count in words_count.items():\n",
        "\n",
        "    # Append if it's more(or equal) to the threshold\n",
        "    if count >= count_threshold :\n",
        "      closed_vocabulary.append(word)\n",
        "\n",
        "  return closed_vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:15:09.151250500Z",
          "start_time": "2023-12-11T21:15:09.135093800Z"
        },
        "id": "vE58X1tkFAzB"
      },
      "outputs": [],
      "source": [
        "def unk_tokenize(tokenized_sentences, vocabulary, unknown_token = \"<unk>\") -> 'list':\n",
        "\n",
        "  # Convert Vocabulary into a set\n",
        "  vocabulary = set(vocabulary)\n",
        "\n",
        "  # Create empty list for sentences\n",
        "  new_tokenized_sentences = []\n",
        "\n",
        "  # Iterate over sentences\n",
        "  for sentence in tokenized_sentences:\n",
        "\n",
        "    # Iterate over sentence and add <unk>\n",
        "    # if the token is absent from the vocabulary\n",
        "    new_sentence = []\n",
        "    for token in sentence:\n",
        "      if token in vocabulary:\n",
        "        new_sentence.append(token)\n",
        "      else:\n",
        "        new_sentence.append(unknown_token)\n",
        "\n",
        "    # Append sentece to the new list\n",
        "    new_tokenized_sentences.append(new_sentence)\n",
        "\n",
        "  return new_tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:15:10.121331400Z",
          "start_time": "2023-12-11T21:15:10.096203800Z"
        },
        "id": "rnDhfonMFAzB"
      },
      "outputs": [],
      "source": [
        "def cleansing(train_data, test_data, count_threshold):\n",
        "\n",
        "  # Get closed Vocabulary\n",
        "  vocabulary = handling_oov(train_data, count_threshold)\n",
        "\n",
        "  # Updated Training Dataset\n",
        "  new_train_data = unk_tokenize(train_data, vocabulary)\n",
        "\n",
        "  # Updated Test Dataset\n",
        "  new_test_data = unk_tokenize(test_data, vocabulary)\n",
        "\n",
        "  return new_train_data, new_test_data, vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:15:25.523673700Z",
          "start_time": "2023-12-11T21:15:11.132252Z"
        },
        "id": "-Skr3J62FAzB"
      },
      "outputs": [],
      "source": [
        "min_freq = 50\n",
        "final_train, final_test, vocabulary = cleansing(train, test, min_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:16:06.049098Z",
          "start_time": "2023-12-11T21:16:06.035258600Z"
        },
        "id": "BcRPjXkcFAzB",
        "outputId": "36f9981b-5f9e-475a-edaf-1715380d73f8"
      },
      "outputs": [],
      "source": [
        "vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:16:07.262290100Z",
          "start_time": "2023-12-11T21:16:07.151164200Z"
        },
        "id": "JLMNBiZ4FAzB",
        "outputId": "14f27a25-87f8-47a4-dd39-1d75eeee0114"
      },
      "outputs": [],
      "source": [
        "final_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:16:08.585849700Z",
          "start_time": "2023-12-11T21:16:08.464580300Z"
        },
        "id": "-sW_mBUBFAzC",
        "outputId": "f2759604-a1d0-4df7-8254-816437eaa523"
      },
      "outputs": [],
      "source": [
        "final_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:16:09.492920Z",
          "start_time": "2023-12-11T21:16:09.467739300Z"
        },
        "id": "PoG4WNtUFAzC"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "def count_n_grams_builtin(data, n, start_token=\"<s>\", end_token=\"<e>\") -> dict:\n",
        "    # Flatten the list of tokenized sentences into a single list of words\n",
        "    all_words = [word for sentence in data for word in sentence]\n",
        "\n",
        "    # Add start and end tokens to the list\n",
        "    all_words = [start_token] * (n - 1) + all_words + [end_token]\n",
        "\n",
        "    # Use NLTK's ngrams function to obtain n-grams\n",
        "    n_grams = list(ngrams(all_words, n))\n",
        "\n",
        "    # Use Counter to count the occurrences of each n-gram\n",
        "    n_gram_counts = Counter(n_grams)\n",
        "\n",
        "    return n_gram_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:16:30.706831300Z",
          "start_time": "2023-12-11T21:16:10.459644500Z"
        },
        "id": "iB2c4FDlFAzC",
        "outputId": "3c1237f7-06c2-4558-9c20-17ba978f9599"
      },
      "outputs": [],
      "source": [
        "ngramstraing =  count_n_grams_builtin(final_train, 2)\n",
        "ngramstraing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:16:35.741231900Z",
          "start_time": "2023-12-11T21:16:35.739305800Z"
        },
        "id": "9rjuYgvJFAzC"
      },
      "outputs": [],
      "source": [
        "def prob_for_single_word(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary_size, k = 1.0) -> 'float':\n",
        "\n",
        "  # Convert the previous_n_gram into a tuple\n",
        "  previous_n_gram = tuple(previous_n_gram)\n",
        "\n",
        "  # Calculating the count, if exists from our freq dictionary otherwise zero\n",
        "  previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
        "\n",
        "  # The Denominator\n",
        "  denom = previous_n_gram_count + k * vocabulary_size\n",
        "\n",
        "  # previous n-gram plus the current word as a tuple\n",
        "  nplus1_gram = previous_n_gram + (word,)\n",
        "\n",
        "  # Calculating the nplus1 count, if exists from our freq dictionary otherwise zero\n",
        "  nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n",
        "\n",
        "  # Numerator\n",
        "  num = nplus1_gram_count + k\n",
        "\n",
        "  # Final Fraction\n",
        "  prob = num / denom\n",
        "  return prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:16:40.483046500Z",
          "start_time": "2023-12-11T21:16:40.478528500Z"
        },
        "id": "GJRh3stXFAzC"
      },
      "outputs": [],
      "source": [
        "def probs(previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0) -> 'dict':\n",
        "\n",
        "  # Convert to Tuple\n",
        "  previous_n_gram = tuple(previous_n_gram)\n",
        "\n",
        "  # Add end and unknown tokens to the vocabulary\n",
        "  vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "\n",
        "  # Calculate the size of the vocabulary\n",
        "  vocabulary_size = len(vocabulary)\n",
        "\n",
        "  # Empty dict for probabilites\n",
        "  probabilities = {}\n",
        "\n",
        "  # Iterate over words\n",
        "  for word in vocabulary:\n",
        "\n",
        "    # Calculate probability\n",
        "    probability = prob_for_single_word(word, previous_n_gram,\n",
        "                                           n_gram_counts, nplus1_gram_counts,\n",
        "                                           vocabulary_size, k=k)\n",
        "    # Create mapping: word -> probability\n",
        "    probabilities[word] = probability\n",
        "\n",
        "  return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:16:45.184924400Z",
          "start_time": "2023-12-11T21:16:45.182857600Z"
        },
        "id": "4eqmGDaNFAzC"
      },
      "outputs": [],
      "source": [
        "def auto_complete(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
        "\n",
        "\n",
        "    # length of previous words\n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "\n",
        "    # most recent 'n' words\n",
        "    previous_n_gram = previous_tokens[-n:]\n",
        "\n",
        "    # Calculate probabilty for all words\n",
        "    probabilities = probs(previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary, k=k)\n",
        "\n",
        "    # Intialize the suggestion and max probability\n",
        "    suggestion = None\n",
        "    max_prob = 0\n",
        "\n",
        "    # Iterate over all words and probabilites, returning the max.\n",
        "    # We also add a check if the start_with parameter is provided\n",
        "    for word, prob in probabilities.items():\n",
        "\n",
        "        if start_with != None:\n",
        "\n",
        "            if not word.startswith(start_with):\n",
        "                continue\n",
        "\n",
        "        if prob > max_prob:\n",
        "\n",
        "            suggestion = word\n",
        "            max_prob = prob\n",
        "\n",
        "    return suggestion, max_prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:16:50.091414200Z",
          "start_time": "2023-12-11T21:16:50.088306800Z"
        },
        "id": "G1ywhnwGFAzD"
      },
      "outputs": [],
      "source": [
        "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
        "\n",
        "    # See how many models we have\n",
        "    count = len(n_gram_counts_list)\n",
        "\n",
        "    # Empty list for suggestions\n",
        "    suggestions = []\n",
        "\n",
        "    # IMP: Earlier \"-1\"\n",
        "\n",
        "    # Loop over counts\n",
        "    for i in range(count-1):\n",
        "\n",
        "        # get n and nplus1 counts\n",
        "        n_gram_counts = n_gram_counts_list[i]\n",
        "        nplus1_gram_counts = n_gram_counts_list[i+1]\n",
        "\n",
        "        # get suggestions\n",
        "        suggestion = auto_complete(previous_tokens, n_gram_counts,\n",
        "                                    nplus1_gram_counts, vocabulary,\n",
        "                                    k=k, start_with=start_with)\n",
        "        # Append to list\n",
        "        suggestions.append(suggestion)\n",
        "\n",
        "    return suggestions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:19:26.266081800Z",
          "start_time": "2023-12-11T21:18:05.239067Z"
        },
        "id": "2gCXXS68FAzD"
      },
      "outputs": [],
      "source": [
        "n_gram_counts_list = []\n",
        "for n in range(1, 6):\n",
        "    n_model_counts = count_n_grams_builtin(final_train, n)\n",
        "    n_gram_counts_list.append(n_model_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T21:24:26.812200600Z",
          "start_time": "2023-12-11T21:24:25.433699400Z"
        },
        "id": "tO0dljfiFAzD",
        "outputId": "ec21ef58-77e1-4d5d-a1a5-085698a1ca79"
      },
      "outputs": [],
      "source": [
        "previous_tokens = [\"i\", \"was\", \"about\"]\n",
        "suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "display(suggestion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yxz2_erDFAzD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
